{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d869eae7",
   "metadata": {},
   "source": [
    "##  Textblob\n",
    "\n",
    "* Textblob --> an open-source python library used to perform NLP activities like Lemmatization, Stemming, Tokenization, Noun Phrase Extraction, POS Tagging, N-Grams, Sentiment Analysis. \n",
    "\n",
    "* It is faster than NLTK, however it does not provide the functionalities like vectorization, dependency parsing.\n",
    "* Installation: pip install textblob"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 56,
   "id": "e3689213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport tensorflow as tf\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\\n\\n# Define data directories and image size\\ntrain_data_dir = 'train_data'\\ntest_data_dir = 'test_data'\\nimage_size = (128, 128)\\n\\n# Data preparation\\ntrain_datagen = ImageDataGenerator(rescale=1./255)\\ntest_datagen = ImageDataGenerator(rescale=1./255)\\n\\ntrain_generator = train_datagen.flow_from_directory(\\n    train_data_dir,\\n    target_size=image_size,\\n    batch_size=32,\\n    class_mode='binary'  # Use 'categorical' for multiple classes\\n)\\n\\ntest_generator = test_datagen.flow_from_directory(\\n    test_data_dir,\\n    target_size=image_size,\\n    batch_size=32,\\n    class_mode='binary'  # Use 'categorical' for multiple classes\\n)\\n\\n# Model architecture\\nmodel = Sequential()\\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_size[0], image_size[1], 3)))\\nmodel.add(MaxPooling2D((2, 2)))\\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\\nmodel.add(MaxPooling2D((2, 2)))\\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\\nmodel.add(MaxPooling2D((2, 2)))\\nmodel.add(Flatten())\\nmodel.add(Dense(512, activation='relu'))\\nmodel.add(Dense(1, activation='sigmoid'))  # Use 'softmax' and change units for multiple classes\\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\\n\\n# Model training\\nmodel.fit(\\n    train_generator,\\n    steps_per_epoch=train_generator.samples // train_generator.batch_size,\\n    epochs=10,  # Adjust the number of epochs based on your dataset and computational resources\\n    validation_data=test_generator,\\n    validation_steps=test_generator.samples // test_generator.batch_size\\n)\\n\\n# Model evaluation\\naccuracy = model.evaluate(test_generator)[1]\\nprint(f'Test Accuracy: {accuracy * 100:.2f}%')\\n\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define data directories and image size\n",
    "train_data_dir = 'train_data'\n",
    "test_data_dir = 'test_data'\n",
    "image_size = (128, 128)\n",
    "\n",
    "# Data preparation\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=32,\n",
    "    class_mode='binary'  # Use 'categorical' for multiple classes\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=32,\n",
    "    class_mode='binary'  # Use 'categorical' for multiple classes\n",
    ")\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_size[0], image_size[1], 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Use 'softmax' and change units for multiple classes\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model training\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    epochs=10,  # Adjust the number of epochs based on your dataset and computational resources\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.samples // test_generator.batch_size\n",
    ")\n",
    "\n",
    "# Model evaluation\n",
    "accuracy = model.evaluate(test_generator)[1]\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 5,
   "id": "0ea0c543",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Install Textblob\n",
    "#pip install nltk\n",
    "#pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51aa56b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to C:\\Users\\AYT-\n",
      "[nltk_data]    |     BRAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to C:\\Users\\AYT-\n",
      "[nltk_data]    |     BRAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to C:\\Users\\AYT-\n",
      "[nltk_data]    |     BRAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to C:\\Users\\AYT-\n",
      "[nltk_data]    |     BRAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to C:\\Users\\AYT-\n",
      "[nltk_data]    |     BRAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to C:\\Users\\AYT-\n",
      "[nltk_data]    |     BRAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to C:\\Users\\AYT-\n",
      "[nltk_data]    |     BRAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to C:\\Users\\AYT-\n",
      "[nltk_data]    |     BRAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to C:\\Users\\AYT-\n",
      "[nltk_data]    |     BRAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to C:\\Users\\AYT-\n",
      "[nltk_data]    |     BRAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to C:\\Users\\AYT-\n",
      "[nltk_data]    |     BRAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to C:\\Users\\AYT-\n",
      "[nltk_data]    |     BRAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to C:\\Users\\AYT-\n",
      "[nltk_data]    |     BRAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to C:\\Users\\AYT-\n",
      "[nltk_data]    |     BRAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to C:\\Users\\AYT-\n",
      "[nltk_data]    |     BRAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to C:\\Users\\AYT-\n",
      "[nltk_data]    |     BRAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to C:\\Users\\AYT-\n",
      "[nltk_data]    |     BRAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to C:\\Users\\AYT-\n",
      "[nltk_data]    |     BRAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\AYT-BRAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to C:\\Users\\AYT-\n",
      "[nltk_data]    |     BRAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to C:\\Users\\AYT-\n",
      "[nltk_data]    |     BRAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\AYT-BRAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5213c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\AYT-BRAIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9fdb42d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Parse and print the translated text\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m translated_text \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslations\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslatedText\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput text in Spanish:\u001b[39m\u001b[38;5;124m\"\u001b[39m, translated_text)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'data'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Google Translate API endpoint\n",
    "url = \"https://translation.googleapis.com/language/translate/v2\"\n",
    "\n",
    "# Your API key\n",
    "api_key = \"KEY\"\n",
    "\n",
    "# Text to translate\n",
    "text = \"Hey Brian, Are you good ?\"\n",
    "\n",
    "# Parameters for the translation request\n",
    "params = {\n",
    "    'q': text,\n",
    "    'source': 'auto',  # Detect source language automatically\n",
    "    'target': 'es',    # Translate to Spanish\n",
    "    'key': api_key      # Your API key\n",
    "}\n",
    "\n",
    "# Make the API request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Parse and print the translated text\n",
    "translated_text = response.json()['data']['translations'][0]['translatedText']\n",
    "print(\"Input text in Spanish:\", translated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fe5f92",
   "metadata": {},
   "source": [
    "#### Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adf80d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "text=\"\"\" ABCD Corp alays values ttheir employees!!!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0757e15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ABCD Corp alays values ttheir employees!!!\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe50b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob=TextBlob(text)\n",
    "blob.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29eab446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"has\")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob('hasss').correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "049a9342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"or\")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "TextBlob('ur').correct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe3704f",
   "metadata": {},
   "source": [
    "### Word Count \n"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 38,

   "id": "f1169c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Sentiment Analysis is a process by which we can find the sentiment of a text. Sentiment can be Positive, Negative or Neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,

   "id": "ad31d646",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob=TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,

   "id": "5b830875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 47,

     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.word_counts[\"analysis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9daa2972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.word_counts[\"process\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1fbf1a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 46,

     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.word_counts[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,

   "id": "4a39c4bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.word_counts[\"Analysis\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3582855",
   "metadata": {},
   "source": [
    "### POS Tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,

   "id": "680e50c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Adam', 'NNP'), ('I', 'PRP'), ('like', 'VBP'), ('to', 'TO'), ('read', 'VB'), ('about', 'IN'), ('NLP', 'NNP'), ('I', 'PRP'), ('work', 'VBP'), ('at', 'IN'), ('ABCD', 'NNP'), ('Corp', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    " \n",
    "text = TextBlob(\"My name is Adam. I like to read about NLP. I work at ABCD Corp.\")\n",
    "print(text.tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,

   "id": "18a7c0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('My', 'PRP$')\n",
      "('name', 'NN')\n",
      "('is', 'VBZ')\n",
      "('Adam', 'NNP')\n",
      "('I', 'PRP')\n",
      "('like', 'VBP')\n",
      "('to', 'TO')\n",
      "('read', 'VB')\n",
      "('about', 'IN')\n",
      "('NLP', 'NNP')\n",
      "('I', 'PRP')\n",
      "('work', 'VBP')\n",
      "('at', 'IN')\n",
      "('ABCD', 'NNP')\n",
      "('Corp', 'NNP')\n"
     ]
    }
   ],
   "source": [
    "new_tuple=[]\n",
    "for i in text.tags:\n",
    "    print(i)\n",
    "    if 'VBP' not in i[1]:\n",
    "        new_tuple.append(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f2a4260c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('My', 'PRP$'),\n",
       " ('name', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('Adam', 'NNP'),\n",
       " ('I', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('read', 'VB'),\n",
       " ('about', 'IN'),\n",
       " ('NLP', 'NNP'),\n",
       " ('I', 'PRP'),\n",
       " ('at', 'IN'),\n",
       " ('ABCD', 'NNP'),\n",
       " ('Corp', 'NNP')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2be7d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "value=''\n",
    "for i in new_tuple:\n",
    "    value=value+\" \" + \"\".join(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d3a2219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' My name is Adam I to read about NLP I at ABCD Corp'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6570dce4",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee75c7a",
   "metadata": {},
   "source": [
    "tokenization?\n",
    "\n",
    "Tokenization is a process of splitting the sentence or corpus into its smalles unit i.e. \"Tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b9a0bd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\n",
    "R is a comprehensive statistical and graphical programming language, which is fast gaining popularity among data analysts. It is free and runs on a variety of platforms, including Windows, Unix, and macOS. It provides an unparalleled platform for programming new statistical methods in an easy and straightforward manner. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d13f6259",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_object = TextBlob(text)\n",
    "# Word tokenization of the sample corpus\n",
    "corpus_words = blob_object.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,

   "id": "a87be64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['R', 'is', 'a', 'comprehensive', 'statistical', 'and', 'graphical', 'programming', 'language', 'which', 'is', 'fast', 'gaining', 'popularity', 'among', 'data', 'analysts', 'It', 'is', 'free', 'and', 'runs', 'on', 'a', 'variety', 'of', 'platforms', 'including', 'Windows', 'Unix', 'and', 'macOS', 'It', 'provides', 'an', 'unparalleled', 'platform', 'for', 'programming', 'new', 'statistical', 'methods', 'in', 'an', 'easy', 'and', 'straightforward', 'manner'])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "45a3f96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4fd6bab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sentences= blob_object.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5528532a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"\n",
       " R is a comprehensive statistical and graphical programming language, which is fast gaining popularity among data analysts.\"),\n",
       " Sentence(\"It is free and runs on a variety of platforms, including Windows, Unix, and macOS.\"),\n",
       " Sentence(\"It provides an unparalleled platform for programming new statistical methods in an easy and straightforward manner.\")]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b598ee43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f396561",
   "metadata": {},
   "source": [
    "#### Pluralization of words using Textblob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "547f7446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Platforms'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word('Platform')\n",
    "w.pluralize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bdf342db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Platformss'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word('Platforms')\n",
    "w.pluralize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e6343f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "platforms\n",
      "sciences\n",
      "communities\n",
      "etcs\n"
     ]
    }
   ],
   "source": [
    "blob = TextBlob(\"Great Learning is a great platform to learn data science. \\n It helps community through blogs, Youtube, GLA,etc.\")\n",
    "for word,pos in blob.tags:\n",
    "    if pos == 'NN':\n",
    "        print (word.pluralize())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc3cb03",
   "metadata": {},
   "source": [
    "#### Lemmatization using Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f9d5ddb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL: Great | LEMMA: Great | STEM: great\n",
      "ORIGINAL: Learning | LEMMA: Learning | STEM: learn\n",
      "ORIGINAL: is | LEMMA: is | STEM: is\n",
      "ORIGINAL: a | LEMMA: a | STEM: a\n",
      "ORIGINAL: great | LEMMA: great | STEM: great\n",
      "ORIGINAL: platform | LEMMA: platform | STEM: platform\n",
      "ORIGINAL: to | LEMMA: to | STEM: to\n",
      "ORIGINAL: learn | LEMMA: learn | STEM: learn\n",
      "ORIGINAL: data | LEMMA: data | STEM: data\n",
      "ORIGINAL: science | LEMMA: science | STEM: scienc\n",
      "ORIGINAL: It | LEMMA: It | STEM: it\n",
      "ORIGINAL: helps | LEMMA: help | STEM: help\n",
      "ORIGINAL: community | LEMMA: community | STEM: commun\n",
      "ORIGINAL: through | LEMMA: through | STEM: through\n",
      "ORIGINAL: blogs | LEMMA: blog | STEM: blog\n",
      "ORIGINAL: Youtube | LEMMA: Youtube | STEM: youtub\n",
      "ORIGINAL: GLA | LEMMA: GLA | STEM: gla\n",
      "ORIGINAL: etc | LEMMA: etc | STEM: etc\n"
     ]
    }
   ],
   "source": [
    "blob = TextBlob(\"Great Learning is a great platform to learn data science. \\n It helps community through blogs, Youtube, GLA,etc.\")\n",
    "words = blob.words\n",
    "\n",
    "for word in words:\n",
    "    print(\"ORIGINAL:\", word, \"| LEMMA:\", word.lemmatize(), \"| STEM:\", word.stem())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1fd4a8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learning'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Word('learning')\n",
    "w.lemmatize(\"n\") ## v here represents verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8d08a8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learn'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Word('learning')\n",
    "w.lemmatize(\"v\") ## v here represents verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e11b2075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'people'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Word('peoples')\n",
    "w.lemmatize(\"n\") ## v here represents verb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01024e7c",
   "metadata": {},
   "source": [
    "#### n-gram in Textblob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fad2e55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Great Learning is a great platform to learn data science. \n",
       " It helps community through blogs, Youtube, GLA,etc.\")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7fbe2828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Great']),\n",
       " WordList(['Learning']),\n",
       " WordList(['is']),\n",
       " WordList(['a']),\n",
       " WordList(['great']),\n",
       " WordList(['platform']),\n",
       " WordList(['to']),\n",
       " WordList(['learn']),\n",
       " WordList(['data']),\n",
       " WordList(['science']),\n",
       " WordList(['It']),\n",
       " WordList(['helps']),\n",
       " WordList(['community']),\n",
       " WordList(['through']),\n",
       " WordList(['blogs']),\n",
       " WordList(['Youtube']),\n",
       " WordList(['GLA']),\n",
       " WordList(['etc'])]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.ngrams(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "67a47ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Great', 'Learning']),\n",
       " WordList(['Learning', 'is']),\n",
       " WordList(['is', 'a']),\n",
       " WordList(['a', 'great']),\n",
       " WordList(['great', 'platform']),\n",
       " WordList(['platform', 'to']),\n",
       " WordList(['to', 'learn']),\n",
       " WordList(['learn', 'data']),\n",
       " WordList(['data', 'science']),\n",
       " WordList(['science', 'It']),\n",
       " WordList(['It', 'helps']),\n",
       " WordList(['helps', 'community']),\n",
       " WordList(['community', 'through']),\n",
       " WordList(['through', 'blogs']),\n",
       " WordList(['blogs', 'Youtube']),\n",
       " WordList(['Youtube', 'GLA']),\n",
       " WordList(['GLA', 'etc'])]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.ngrams(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3346e17e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Great', 'Learning', 'is']),\n",
       " WordList(['Learning', 'is', 'a']),\n",
       " WordList(['is', 'a', 'great']),\n",
       " WordList(['a', 'great', 'platform']),\n",
       " WordList(['great', 'platform', 'to']),\n",
       " WordList(['platform', 'to', 'learn']),\n",
       " WordList(['to', 'learn', 'data']),\n",
       " WordList(['learn', 'data', 'science']),\n",
       " WordList(['data', 'science', 'It']),\n",
       " WordList(['science', 'It', 'helps']),\n",
       " WordList(['It', 'helps', 'community']),\n",
       " WordList(['helps', 'community', 'through']),\n",
       " WordList(['community', 'through', 'blogs']),\n",
       " WordList(['through', 'blogs', 'Youtube']),\n",
       " WordList(['blogs', 'Youtube', 'GLA']),\n",
       " WordList(['Youtube', 'GLA', 'etc'])]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.ngrams(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e22ce261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Great', 'Learning', 'is', 'a']),\n",
       " WordList(['Learning', 'is', 'a', 'great']),\n",
       " WordList(['is', 'a', 'great', 'platform']),\n",
       " WordList(['a', 'great', 'platform', 'to']),\n",
       " WordList(['great', 'platform', 'to', 'learn']),\n",
       " WordList(['platform', 'to', 'learn', 'data']),\n",
       " WordList(['to', 'learn', 'data', 'science']),\n",
       " WordList(['learn', 'data', 'science', 'It']),\n",
       " WordList(['data', 'science', 'It', 'helps']),\n",
       " WordList(['science', 'It', 'helps', 'community']),\n",
       " WordList(['It', 'helps', 'community', 'through']),\n",
       " WordList(['helps', 'community', 'through', 'blogs']),\n",
       " WordList(['community', 'through', 'blogs', 'Youtube']),\n",
       " WordList(['through', 'blogs', 'Youtube', 'GLA']),\n",
       " WordList(['blogs', 'Youtube', 'GLA', 'etc'])]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.ngrams(n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4679995b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
